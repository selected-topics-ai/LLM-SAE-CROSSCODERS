{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sae_lens import SAE\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:27:16.341700Z",
     "start_time": "2025-04-22T20:27:16.309790Z"
    }
   },
   "cell_type": "code",
   "source": "notebook_login()",
   "id": "177c9a757c2bb6e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98205001d1a44374a3042c514feda190"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Привет. Я попытался сделать задание через API нейропедии, но не мог получить ответы от модели поскольку постоянно упирался в лимит токенов. Я поднял диписик на A100 и использовал SAE с нейропедии для стиринга",
   "id": "2bdcddbf0b5ee29e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:02:14.302714Z",
     "start_time": "2025-04-22T19:02:14.296545Z"
    }
   },
   "cell_type": "code",
   "source": "fractions_df = pd.read_json('../fractions/fractions.json')",
   "id": "7829c2c446c6049b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:17:11.494208Z",
     "start_time": "2025-04-22T21:16:23.848935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")"
   ],
   "id": "c69102ddcb08e045",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 18.92s/it]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:20:51.140214Z",
     "start_time": "2025-04-22T21:20:51.136131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_json('../fractions/fractions.json')\n",
    "df['questions'] = df['questions'].map(lambda x: (x.replace(\"What is \", \"Solve folowing example. Write  only fraction.\\n\").replace(\"?\", \" = \")))"
   ],
   "id": "6101f4cf3facc466",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conversations = []\n",
    "\n",
    "for message in df['questions'].tolist():\n",
    "  conversation = []\n",
    "  conversation.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": message\n",
    "  })\n",
    "  conversations.append(conversation)"
   ],
   "id": "a022313e8fdfde61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chat_conv_formatted = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(chat_conv_formatted, return_tensors=\"pt\", padding=True)\n",
    "inputs.to(\"cuda:0\")"
   ],
   "id": "1ba72ba7a28032d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:38:42.102025Z",
     "start_time": "2025-04-22T21:20:54.129982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"llama_scope_r1_distill\",\n",
    "    sae_id=\"l15r_800m_slimpajama\",\n",
    "    device=\"cuda:0\")\n",
    "\n",
    "\n",
    "def generate_with_sae(model,\n",
    "                      sae: SAE,\n",
    "                      layer: int,\n",
    "                      feature_index: int,\n",
    "                      alpha: float,\n",
    "                      do_sample: bool=True,\n",
    "                      temperature: float=0.6,\n",
    "                      max_len: int=512,\n",
    "                      repetition_penalty:float=1.0,\n",
    "                      normalize_new_hidden=False,\n",
    "                      eos_token_id=128001):\n",
    "\n",
    "    assert alpha >= 0\n",
    "\n",
    "    def gather_act_hook(mod, inputs, outputs):\n",
    "\n",
    "        h = outputs[0]\n",
    "\n",
    "        try:\n",
    "\n",
    "            h = sae.encode(h)\n",
    "            steering = torch.zeros_like(h)\n",
    "            steering[:, :, feature_index] = alpha\n",
    "\n",
    "            if normalize_new_hidden:\n",
    "                h_steer = h + steering\n",
    "                h_steer = (h_steer / h_steer.norm(dim=-1, keepdim=True)) * h.norm(dim=-1, keepdim=True)\n",
    "            else:\n",
    "                h_steer = h + steering\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            h_steer = outputs[0]\n",
    "\n",
    "        h_steer = sae.decode(h_steer)\n",
    "\n",
    "        return (h_steer,)\n",
    "\n",
    "    hook = model.model.layers[layer].register_forward_hook(gather_act_hook)\n",
    "\n",
    "    model = model.to(\"cuda:0\")\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      outputs = model.generate(**inputs,\n",
    "                              max_length=max_len,\n",
    "                              temperature=temperature,\n",
    "                              do_sample=do_sample,\n",
    "                              repetition_penalty=repetition_penalty,\n",
    "                              use_cache=False,\n",
    "                              eos_token_id=eos_token_id)\n",
    "    hook.remove()\n",
    "\n",
    "    return outputs"
   ],
   "id": "395f40dc54a5954",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = inputs.to(\"cuda:0\")\n",
    "steer_strengths = [5, -5, 1, -1, 0, -10, 10]\n",
    "\n",
    "for steer_strength in tqdm(steer_strengths):\n",
    "\n",
    "  encoded_outputs = generate_with_sae(\n",
    "      model=model,\n",
    "      sae=sae,\n",
    "      alpha=steer_strength,\n",
    "      layer=15,\n",
    "      feature_index=30939,\n",
    "  )\n",
    "\n",
    "  decoded_outputs = []\n",
    "  for output in encoded_outputs:\n",
    "    decoded_outputs.append(tokenizer.decode(output, skip_special_tokens=False))\n",
    "\n",
    "  str_steer_strength = str(steer_strength).replace(\"-\", \"minus_\")\n",
    "  df[f'answers_for_steer_{str_steer_strength}'] = decoded_outputs"
   ],
   "id": "490068c51b34fd6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
